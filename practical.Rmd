---
title: "Statistical Analysis of Biological Data"
author: "Mark Dunning"
date: '`r format(Sys.time(), "Last modified: %d %b %Y")`'
output: 
  html_notebook: 
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Acknowledgement

Some of these materials are based on previous courses at Cancer Research Uk Cambridge Institute.

- [Introduction to Statistical Analysis](http://bioinformatics-core-shared-training.github.io/IntroductionToStats/)
- [Further Statistical Analysis in R](https://rawgit.com/bioinformatics-core-shared-training/intermediate-stats/master/manual.pdf)

# Part I

## How to test for normality

We will read some example data to illustrate how one would test for a normally-distributed variable. This property is important as it influences which test we should use.

One of the best ways of displaying data is by using a graph. Graphs can make both simple and complex data easier to understand by making it easier to spot trends and patterns. We can use plots to view the distribution of our data (minimum, maximum, mid-point, spread etc) and to ensure that the values in our dataset seem realistic. Many statistical tests rely on the assumption that the data are normally distributed, which can be assessed using histograms or box plots (more later)


First we will load the `tidyverse` set of packages that are recommended for data manipulation and visualisation. The data for this first section are to be found in the file `normal_example.csv` in the `data` folder. You will need to specify the file path accordingly.

```{r message=FALSE}
library(tidyverse)
df1 <- read_csv("data/normal_example.csv")
```

We can inspect the data in RStudio and discover that it consists of a single column called `x` that comprises numeric observations

```{r}
View(df1)
```

Various graphical methods are available to assess the distrbution. The first of which is a *histogram*. Here, the data are split into "bins" (`ggplot2` choses the number of bins) and the value on the `y` axis corresponds to the number of observations in that bin. The user only has to specify the variable to be plotted, and `ggplot2` takes care of the binning. From this plot we can judge what the average value of the data is and the spread.

```{r}
ggplot(df1, aes(x=x)) + geom_histogram()
```

A similar option is to produce a density curve. Here the y-axis is the *density* of a particular value.

```{r}
ggplot(df1, aes(x = x)) + geom_density()
```

A box plot is an excellent way of displaying continuous data when you are interested in the spread of your data. The box of the box plot corresponds to the lower and upper quartiles of the respective observations and the bar within the box, the median. The whiskers of the box plot correspond to the distance betweenthe lower/upper quartile and thesmallerof: the smaller/largest measurement OR 1.5 times the inter quartile range.A disadvantage of the box plot is that you don’t see the exact data points. However, box plots are very useful in large datasets where plotting all of the data may give an unclear picture of the shape of your data.

```{r}
ggplot(df1, aes(x="",y=x)) + geom_boxplot()
```

A *violin plot* is sometimes used in conjunction with the boxplot to show density information.

```{r}
ggplot(df1, aes(x="",y=x))  + geom_violin() + geom_boxplot()
```


Finally, we have a "*qq-plot*" which allows to compare the quantiles of our dataset against a theoretical normal distribution. If the majority of points lie on a diagonal line then the data are approximately normal.

```{r}
ggplot(df1,aes(sample=x)) + geom_qq() + geom_qq_line(col="red")
```

These graphical methods are by far the easiest way to assess if a given dataset is normally-distributed. For "real-life" data, the results are unlikely to give a perfect plot, so some degree of judgement and prior experience with the data type are required. 


In fact, these data were simulated from a normal distribution. Now lets read some more realistic data

```{r}
df2 <- read_csv("data/toy_example.csv")
df2
```
******
******
******

#### Exercise

- Are the datasets `x` and `y`, contained in the data frame `df2` normally-distributed or not? Use the plots introduced above to justify your answer.

******
******
******


## Tests for normality

There are a couple of methods for testing whether variables are normally-distributed or not. If the p-value is sufficiently small then we conclude that the data are not normally distributed. However, some statisticians prefer to use graphical methods and their intuition about the data or prior knowledge of the data type (e.g. some measures are generally believed to be normally-distributed)

```{r}

#shapiro test
#p<0.05 ...difference between data and normality..data not normal
#p>0.05 ...no diff between data and normality ..data normally distributed

shapiro.test(df1$x)

```

```{r}
#kolomogorov-Smirnov test
ks.test(df1$x,"pnorm", mean=mean(df1$x), sd=sd(df1$x))

#what test to choose
#kolomogorov-Smirnov test...sample >=1000
#Shapiro....smaller samples but more strict(<5000)
```





## Summary Statistics

In the [accompanying R course](http://sbc.shef.ac.uk/r-crash-course/) we have seen how to produce summary statistics of columns in a dataset. For a dataset that is normally-distributed, appropriate measures of the average and variability are the mean and standard deviation. Both these functions are available within R and can be used in conjunction with the `summarise` function in `tidyverse`.

```{r}
summarise(df1, Mean = mean(x), 
          Var= var(x),
          SD = sd(x),
          SE = sd(x) / sqrt(n()))
```

The `psych` package provides a convenient function `describe` for generating these statistics, and more

```{r}
library(psych)
describe(df2)

```


******
******
******

#### Exercise

1- Read the excel file called ‘EX Biostat P1’ into R 
2- Identify if the age and hospitalization days are normally distributed
3- Use the appropriate descriptive statistics [mean±SD or median (IQ range)] for each variable 


******
******
******

# Part 2 - Contingency tables

When working with categorical variables, we are usually interested in thefrequencies of the different categories in our sample. To display data for two or more categorical variables, cross-tabulations, or contingency tables, are commonly used - with 2 x 2 tables being the simplest. We can then test whether there is anassociation between the row factor and the column factor by a *chi-squared test* or a *Fisher’s exact test*.

To demonstrate the analysis of contingency tables we will use a dataset provided with the `vcd` package. You will need to install this package using the R command

```{r eval=FALSE}
install.packages("vcd")
```


The data frame `Arthritis` should then be accessible which is described as:- 

> Data from Koch & Edwards (1988) from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis.

```{r}
#let's use the'Arthritis' dataset in the 'vcd' package 
library(vcd)
Arthritis
```

<div class="alert alert-warning">

**Question: What do you think the null hypothesis for this dataset might be? How would you test it and what would your summary statistics be?**

</div>

We can use the `summarise` function to obtain counts of a single variable with the special `n()` function. Grouping using the `group_by` function determines which variables are counted.



```{r}
##one way table (count of one variable)
group_by(Arthritis, Sex) %>% 
  summarise(N = n())
```

Further manipulation of this table will give proportions (which will ultimately be used in the statistical testing)

```{r}
#to get proportion of males and females
group_by(Arthritis, Sex) %>% 
  summarise(N = n()) %>% 
  mutate(prop = prop.table(N))
```

With `group_by` we can use two variables to summarise by. The order in which they are specified will dictate how the proportions are calculated. In the below example, proportions are defined in terms of Sex first, and then Improved.

```{r}
group_by(Arthritis, Sex, Improved) %>% 
  summarise(N = n()) %>% 
  mutate(prop = prop.table(N))

```

Compare to the next block of code:-

```{r}
group_by(Arthritis, Improved, Sex) %>% 
  summarise(N = n()) %>% 
  mutate(prop = prop.table(N))
```

If we simply want the proportions over the whole dataset, we could divide by the total number of rows

```{r}
group_by(Arthritis, Sex, Improved) %>% 
  summarise(N = n()) %>% 
  mutate(prop = N/84)
```

The function `chisq.test` can be used to assess whether differences in proportions are significant or not. Unfortunately it requires data to be presented in a different format; that of a contingency table with rows and columns for each variable being investigated. We actually don't need to calculate the proportions; R will do this for us.

For such a transformation we can use the `spread` function from `tidyverse` which will transform a *long* table into a *wide* one.

```{r}
group_by(Arthritis, Improved, Treatment) %>% 
  summarise(N = n()) %>% 
  spread(Improved, N)
```

The `chisq.test` function will provide a p-value that we can use to assess significance.

```{r}
group_by(Arthritis, Improved, Treatment) %>% 
  summarise(N = n()) %>% 
  spread(Improved, N) %>%
  select(-Treatment) %>% 
  chisq.test()
```

Thechi-squaredtest is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. 

Alternatively, Fisher’s exact test can be used


```{r}
group_by(Arthritis, Improved, Sex) %>% 
  summarise(N = n()) %>% 
  spread(Improved, N) %>%
  select(-Sex) %>% 
  fisher.test()
```


******
******
******

#### Exercise

******
******
******

- Read the excel file called `EX Biostat P2.xls` into R 

- Make a cross table named `GenderGrade` showing the gender in the rows and tumor grade in the columns

- Determine the percentage of females with Grade III tumor in the total sample 

- Determine the percentage of Grade III tumors within females only

- Use the appropriate test to check if the tumor grade depends on the gender 

******
******
******

# Part 3 - Comparing 2 or more continuous variables

In this part we will show how to perform tests to compare 2 (or more) continuous variables. We will simulate some example data to be used for the testing. Lets' imagine that patients belonging to three different groups (A, B or C) have been measured for their risk of developing different cancer types.

```{r}
set.seed(20190717)

#make some data
group <- sample(c("A","B","C"),50,TRUE)
lung <- rnorm(50,5,6)
ovarian <- rnorm(50,7.8,4.5)
breast <- rnorm(50,65,16.7)

rates <- data.frame(group,lung,ovarian,breast) 
# in excel the grouping variable must be 'character' not numbers
head(rates)
```

The data are presented in a table with one column for each season and a measurement of either E, F or G. Data in this form are sometimes referred to as *wide* data and are not especially convenient for analysis with `dplyr` and `ggplot2`. 

In the first stage of our analysis we would like to visualise and compare the distribution of our variables, but this is not straightforward with one line of `tidyverse` code.

It turns out that we can convert the data into *long* format for analysis. The contents of the data frame are the same, just transformed into a different shape. Arguably, these data are less convenient for humans to visualise, but allows the data to be manipulated using the tools we are familiar with.

The `tidyr` package provides a function called `gather` that will convert from wide to long data.

```{r eval=FALSE}
tidyr::gather(rates, key=variable, value=value,-group)
```

The consequence is that the new data frame can be plotted using `ggplot` in a familiar way. With the "piping" features of `tidyverse` we actually don't have to make permanent changes to the underlying data. A boxplot of the data can be created as follows:-

```{r}
rates %>% gather(variable, value, -group) %>%
  ggplot(aes(x = variable, y = value)) + geom_boxplot() + geom_jitter(width=0.1)
```


## The Independant t test.......TWO independant groups 
A two-sample t-test should be used if you want to compare the measurements of two populations. There are two types of two-sample t-test:independent (unpaired) and paired (dependent). To make the correct choice, you need to understand your underlying data. An independent two-sample t-test is used when the two samples are independent of each other, e.g. *comparing the mean response of two groups of patients on treatment vs. control in a clinical trial*. As the name suggests,a paired two-sample t-test is used when the two samples are paired, e.g. *comparing the mean blood pressure of patients before and after treatment* (two measurements per patient).

Lets consider that we want to compare the rates of a particular cancer (say breast) between patient groups A and B. Here we have two groups and these can be treated as *independant* variables as different patients belong to the two groups.

The *null hypothesis* for such a test would be that the rate of breast cancer is the same between groups A and B. We seek to evidence to reject this hypothesis by calculating a test statistic. Firstly, as our inital data frame contains all groups and diseases we need to filter to the appropriate rows.

```{r}
rates1 <- rates %>% gather(variable, value, -group) %>%
  filter(group %in% c("A","B"),variable=="breast") 
rates1

```

For such a test we need to test if the variances in the two groups are roughly the same. The easiest means of doing this is graphically:-

```{r}
ggplot(rates1, aes(x = group,y=value)) + geom_boxplot()
```

The boxplot in this case is somewhat misleading, as we can calculate that the variance in group B is nearly double that of A.

```{r}
rates1 %>% group_by(group) %>% 
  summarise(Variance=var(value))

```


We can now use the `t.test` function to perform an *independant* test. This will provide us with a *t* test statistic and a p-value. **However, it is up to us to interpret the p-value**.

The first argument to `t.test` is the *R formula* notation for the test being performed. It allows us to define which columns in our dataset are the numeric and grouping variables in the test.

This function allows various type of test to be performed by changing the appropriate arguments (see the help for `t.test` for details (`?t.test`)). For instance, we can tell the test that we believe our variances are not equal.

```{r}
t.test(value ~ group, data=rates1 ,var.equal = FALSE)
```

We can see that the t-statistic we observe is consistent with the null hypothesis, that the rates of breast cancer are the same for groups A and B. That is, the probability of observing a t-statistic of 1.52 or more, or -1.52 or less, is quite high.

This is not a significant result (p>0.05), so there is no evidence of a difference in breast cancer rates in groups A and B

#### Exercise

******
******
******

Compare the rates of breast and lung cancer for patients in group A. Is there evidence of a significant difference?

******
******
******


## The Paired t test.......TWO dependant groups 

In the previous test we assumed no relationship between the values being tested. However, we know from the experimental design that observations are made *for the same patient* for breast and lung cancer. If we want to test for differences between these variables, we can take the pairing into account. As you will see from the output, the hypothesis testing is performed on the *differences* of the two groups.

```{r}
rates2 <- rates %>% gather(variable, value, -group) %>%
  filter(group =="A",variable %in% c("lung","breast")) 
```

```{r}
ggplot(rates2, aes(x = variable,y=value)) + geom_boxplot()
```

```{r}
t.test(value~variable,data=rates2, paired = TRUE)
```


At the moment we are only consider analyses where we have two groups (/samples) to compare. One technique to compare to multiple groups to is to apply multiple t-tests to the dataset. Later we will discover how `ANOVA` can be used on the same data.

```{r}
rates3 <- rates %>% gather(variable, value, -group) %>%
  filter(group=="A")

pairwise.t.test(rates3$value,rates3$variable)  
```


## Non- Parametric alternatives (Wilcoxon test)

Being able to use the `t-test` relies on the your data being normally-distributed. If we do not sufficient confidence in this assumption, there are different statistical tests that can be applied. Rather than calculating and comparing the *means* and *variances* of different groups they are *rank-based* methods. However, they still come with a set of assumptions and involve the generation of test statistics and p-values.

### Independant samples = Wilcoxon rank sum test (Mann Whitney U test)

This test has many different names including the Wilcoxon, Wilcoxon two sample test, Wilcoxon-Mann-Whitney, Wilcoxon rank sum and the Mann-Whitney-U test. However, this test should notbe confused with the Wilcoxon signed rank test. To avoid confusion this test is usually referred to as the Mann-Whitney Utest, which is used when the dependent variable to be examined is continuous but the assumptions for parametric tests are violated.

The assumptions of the Mann-Whitney U are as follows:

1.The dependent variable is ordinal or continuous.
2.The data consist of a randomly selected sample of independent observations from two independent groups.
3.The dependent variables for the two independent groups share a similar shape.

Fortunately, the R programmers have made the function to do a wilcox test similar to doing a t-test. **The difficulty is in choosing the correct to apply - which R will not advise you on**.

Let's go back to our example of comparing breast cancer rates between groups A and B. The equivalent non-parametric version of the test we performed before is:-

```{r}
wilcox.test(value~group, data=rates1)
```

Similarly, to compare breast and lung cancer rates in a non-parametric fashion we can do:-

```{r}
wilcox.test(value~variable,data=rates2)
```

### Paired samples = Wilcoxon signed rank test

The `wilcox.test` is flexible in much the same way that `t.test` in. We can switch to applying a paired test by adding the argument `paired=TRUE`.

```{r}
wilcox.test(value~variable,data=rates2,paired=TRUE)
```


## Compare between MORE THAN TWO GROUPS 
#### Parametric (ANOVA)

The two-sample t-test is useful when we have just two groups of continuous data to compare. When we want to compare more than two groups, a one-way ANOVA can be used to simultaneously compare all groups, rather than carrying out several individual two-sample t-tests.  The main advantage of doing this is that it reduces the number of tests being carried out, meaning that the type I error rate (the probability of seeing a significant result just by chance) does not become inflated. 

A one-way ANOVA compares group means by partitioning the variation in the data into *between group variance* and *within group variance*.


```{r}
rates3 <- gather(rates,variable, value, -group) %>%
  filter(group=="A")
```


```{r}
anova <- aov(value ~ variable, data=rates3)
anova
summary(anova)
```

When the F-test provides a significant result it tells us that there is at least on difference in the groups. However, it does not tell us which group is different. For this, we can apply a "post-hoc test" such as the Tukey test.


```{r}
TukeyHSD(anova)
```




### Independant samples = kruskal Wallis

```{r}


```




```{r}
kruskal.test(value ~ as.factor(variable), data=rates3)
```


## Two-way ANOVA

```{r}
medulloblastoma <- readxl::read_xlsx("data/Medulloblastoma.xlsx")
```

```{r}
twoway <- aov(`C-myc` ~ Site + Size,data=medulloblastoma)
summary(twoway)
```

```{r}
ggplot(medulloblastoma,aes(x = Metastasis, y=`C-myc`)) + geom_boxplot()
```

